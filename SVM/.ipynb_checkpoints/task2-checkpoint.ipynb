{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "# import tensorflow as tf \n",
    "import sklearn as sk \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, skew \n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt  # Matlab-style plotting\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           ID  Elevation  Aspect  Slope  Horizontal_Distance_To_Hydrology  \\\n",
      "0   452309010       2468      76     10                               108   \n",
      "1   754045784       2052      65     24                               150   \n",
      "2  1347023875       2570     235     32                               371   \n",
      "\n",
      "   Vertical_Distance_To_Hydrology  Horizontal_Distance_To_Roadways  \\\n",
      "0                              14                              201   \n",
      "1                              31                              518   \n",
      "2                             -29                              726   \n",
      "\n",
      "   Hillshade_9am  Hillshade_Noon  Hillshade_3pm  \\\n",
      "0            233             220            118   \n",
      "1            234             184             68   \n",
      "2            150             248            221   \n",
      "\n",
      "   Horizontal_Distance_To_Fire_Points  Soil_Type  From_Cache_la_Poudre  \n",
      "0                                1100       2703                     0  \n",
      "1                                 618       2717                     1  \n",
      "2                                1243       2705                     0  \n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 'Soil_Type_2702', 'Soil_Type_2703', 'Soil_Type_2704', 'Soil_Type_2705', 'Soil_Type_2706', 'Soil_Type_2717', 'Soil_Type_4703', 'Soil_Type_4704', 'Soil_Type_4758', 'Soil_Type_5101', 'Soil_Type_6101', 'Soil_Type_6102', 'Soil_Type_7102', 'Soil_Type_7756', 'Soil_Type_7757']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/muzhouliu/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/Users/muzhouliu/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "dataset_train = pd.read_csv('train.csv')\n",
    "print(dataset_train.head(3))\n",
    "X = dataset_train.iloc[:, 1:12]\n",
    "n = X.shape[0]\n",
    "# print(X.head(3))\n",
    "Y = dataset_train.iloc[:, 12]\n",
    "# test train split\n",
    "# # =============================================================================\n",
    "dataset_test = pd.read_csv('test.csv')\n",
    "Xtest = dataset_test.iloc[:, 1:12]\n",
    "\n",
    "X_all = pd.concat([X,Xtest], axis=0)\n",
    "\n",
    "X_all['Soil_Type'] = X_all['Soil_Type'].astype('category') \n",
    "X_all = pd.get_dummies(X_all)\n",
    "\n",
    "\n",
    "\n",
    "scaler = StandardScaler().fit(np.array(X.iloc[:,:9]))\n",
    "X_std = scaler.transform(np.array(X.iloc[:,:9]))\n",
    "X_std = pd.DataFrame(X_std)\n",
    "\n",
    "X_std_full = pd.concat([X_std,X_all.iloc[:n,10::]],axis = 1)\n",
    "print(list(X_std_full))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_std_full, Y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_std_full, Y, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV # Search over specified parameter values for an estimator.\n",
    "from sklearn.model_selection import RandomizedSearchCV # Search over specified parameter values for an estimator.\n",
    "from sklearn.model_selection import ShuffleSplit # Random permutation cross-validator\n",
    "from sklearn.svm import SVC\n",
    "from AUC import AUC\n",
    "classifier = SVC(kernel = 'rbf', random_state=42)\n",
    "cv_sets = ShuffleSplit(random_state = 4) # shuffling our data for cross-validation\n",
    "parameters = {'gamma':(np.arange(1,40)/20).tolist(),\n",
    "\t\t\t  'degree':(np.arange(1,40)/2).tolist(),\n",
    "\t\t\t  'C': (np.arange(1,40)/5).tolist(),\n",
    "\t\t\t  'coef0' :(np.arange(1,40)/20).tolist()}\n",
    "scorer = make_scorer(AUC,greater_is_better =True)\n",
    "n_iter_search = 10\n",
    "grid_obj = RandomizedSearchCV(classifier, \n",
    "                              parameters, \n",
    "                              n_iter = n_iter_search, \n",
    "                              scoring = scorer, \n",
    "                              cv = cv_sets,\n",
    "                              random_state= 99)\n",
    "grid_fit = grid_obj.fit(X_train, y_train)\n",
    "rf_opt = grid_fit.best_estimator_\n",
    "print(grid_fit.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = pd.read_csv('test.csv')\n",
    "print(dataset_train.head(3))\n",
    "Xtest = dataset_test.iloc[:, 1:12]\n",
    "# print(Xtest.head(3))\n",
    "Xtest['Soil_Type'] = Xtest['Soil_Type'].astype('category') \n",
    "\n",
    "scaler = StandardScaler().fit(np.array(X.iloc[:,[0,1,2,3,4,5,6,7,8,9]]))\n",
    "Xtest_std = scaler.transform(np.array(Xtest.iloc[:,[0,1,2,3,4,5,6,7,8,9]]))\n",
    "Xtest_std = pd.DataFrame(Xtest_std)\n",
    "Xtest_std_full = pd.concat([Xtest_std,X_all.iloc[n::,10::]],axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "ids = dataset_test['ID']\n",
    "predictions = svclass(X_std_full,Y,Xtest_std_full)\n",
    "\n",
    "# output = pd.DataFrame({ 'ID' : ids, 'From_Cache_la_Poudre': predictions })\n",
    "# output.to_csv('svm_submission.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
