{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muzhou Liu muzhouliu@wustl.edu 465729\n",
    "\n",
    "Yidi Zhang yidi.zhang@wustl.edu 465621"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import tensorflow as tf \n",
    "import sklearn as sk \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, skew \n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt  # Matlab-style plotting\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import make_scorer\n",
    "color = sns.color_palette()\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          ID  Elevation       ...        Soil_Type      Cover_Type\n",
      "0   40387826       2789       ...             4704  Lodgepole Pine\n",
      "1  139907508       2075       ...             2702  Ponderosa Pine\n",
      "2  244759849       2476       ...             2705  Ponderosa Pine\n",
      "\n",
      "[3 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "dataset_train = pd.read_csv('../input/train.csv' )\n",
    "print(dataset_train.head(3))\n",
    "X = dataset_train.iloc[:, 1:13]\n",
    "n = X.shape[0]\n",
    "# print(X.head(3))\n",
    "Y = dataset_train.iloc[:, 13]\n",
    "# test train split\n",
    "\n",
    "\n",
    "X['sin'] = np.sin( X['Aspect'])\n",
    "X['cos'] = np.cos( X['Aspect'])\n",
    "X = X.drop(columns =['Aspect'])\n",
    "\n",
    "\n",
    "X['climate_zone'] = X['Soil_Type'].astype(str).str[0] \n",
    "X['geologic'] = X['Soil_Type'].astype(str).str[1] \n",
    "X['soil'] = X['Soil_Type'] % 100\n",
    "X['soil'] = X['soil'].astype('category')\n",
    "X['climate_zone'] = X['climate_zone'].astype('category')\n",
    "X['geologic'] = X['geologic'].astype('category')\n",
    "X = X.drop(columns =['Soil_Type'])\n",
    "\n",
    "\n",
    "X['Wilderness_Area1'] = X['Wilderness_Area'].astype('category')\n",
    "X = X.drop(columns =['Wilderness_Area'])\n",
    "\n",
    "X.iloc[:,:11] = X.iloc[:,:11].astype('float32')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype float32 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:2: DataConversionWarning: Data with input dtype float32 were all converted to float64 by StandardScaler.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "minmax = StandardScaler().fit(X.iloc[:,:11])\n",
    "X_mm = minmax.transform(X.iloc[:,:11]) \n",
    "\n",
    "\n",
    "X.iloc[:,:11] = X_mm\n",
    "\n",
    "X_mm_oh = pd.get_dummies(X)\n",
    "\n",
    "\n",
    "\n",
    "y = pd.get_dummies(Y).values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, axis=1))\n",
    "            / predictions.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-6373ada37d34>:27: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-5-6373ada37d34>:44: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "step:0 loss:1.989471 accuracy: 12.89\n",
      "step:100 loss:0.632131 accuracy: 72.56\n",
      "step:200 loss:0.546029 accuracy: 76.09\n",
      "step:300 loss:0.357997 accuracy: 85.05\n",
      "step:400 loss:0.356696 accuracy: 85.35\n",
      "step:500 loss:0.199230 accuracy: 92.24\n",
      "step:600 loss:0.092440 accuracy: 96.64\n",
      "step:700 loss:0.090330 accuracy: 96.73\n",
      "step:800 loss:0.170425 accuracy: 93.52\n",
      "step:900 loss:0.039745 accuracy: 98.85\n",
      "step:1000 loss:0.019853 accuracy: 99.57\n",
      "step:1100 loss:0.027846 accuracy: 99.27\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.0005\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 1024 # 1st layer number of neurons\n",
    "n_hidden_2 = 2048 # 2nd layer number of neurons\n",
    "n_hidden_3 = 3036\n",
    "n_hidden_4 = 4096\n",
    "n_hidden_5 = 1024\n",
    "n_hidden_6 = 1024\n",
    "n_hidden_7 = 512\n",
    "n_hidden_8 = 512\n",
    "n_hidden_9 = 384\n",
    "n_hidden_10 = 384\n",
    "n_hidden_11 = 256\n",
    "n_input = 51 # data has 51 imput \n",
    "n_output =7\n",
    "\n",
    "\n",
    "X1 = tf.placeholder(\"float\", [None, n_input])\n",
    "Y1 = tf.placeholder(\"float\", [None, n_output])\n",
    "\n",
    "\n",
    "\n",
    "layer_1 = tf.layers.dense(X1,n_hidden_1 , tf.nn.tanh)\n",
    "layer_2 = tf.layers.dense(layer_1, n_hidden_2, tf.nn.tanh)\n",
    "layer_3 = tf.layers.dense(layer_2, n_hidden_3, tf.nn.tanh)\n",
    "layer_4 = tf.layers.dense(layer_3, n_hidden_4, tf.nn.tanh)\n",
    "layer_5 = tf.layers.dense(layer_4, n_hidden_5, tf.nn.tanh)\n",
    "layer_6 = tf.layers.dense(layer_5, n_hidden_6, tf.nn.tanh)\n",
    "layer_7 = tf.layers.dense(layer_6, n_hidden_7, tf.nn.tanh)\n",
    "layer_8 = tf.layers.dense(layer_7, n_hidden_8, tf.nn.tanh)\n",
    "layer_9 = tf.layers.dense(layer_8, n_hidden_6, tf.nn.tanh)\n",
    "layer_10 = tf.layers.dense(layer_9, n_hidden_7, tf.nn.tanh)\n",
    "layer_11 = tf.layers.dense(layer_10, n_hidden_8, tf.nn.tanh)\n",
    "\n",
    "logits = tf.layers.dense(layer_11,n_output)\n",
    "\n",
    "\n",
    "\n",
    "loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels=Y1))  # compute cost\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "train_op = optimizer.minimize(loss)\n",
    "\n",
    "predictions = tf.nn.softmax(logits)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sess = tf.Session()                                 # control training and othersb\n",
    "sess.run(tf.global_variables_initializer())         # initialize var in graph\n",
    "\n",
    "\n",
    "for step in range(20001):\n",
    "        _, l, pred = sess.run([train_op, loss, predictions], {X1: X_mm_oh, Y1: y})\n",
    "        if accuracy(pred,y) >= 99.93:\n",
    "            break\n",
    "        if step % 100 == 0:\n",
    "                            \n",
    "            print('step:{} loss:{:.6f} accuracy: {:.2f}'.format(\n",
    "                    step, l, accuracy(pred, y)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          ID  Elevation       ...        Soil_Type      Cover_Type\n",
      "0   40387826       2789       ...             4704  Lodgepole Pine\n",
      "1  139907508       2075       ...             2702  Ponderosa Pine\n",
      "2  244759849       2476       ...             2705  Ponderosa Pine\n",
      "\n",
      "[3 rows x 14 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:29: DataConversionWarning: Data with input dtype float32 were all converted to float64 by StandardScaler.\n"
     ]
    }
   ],
   "source": [
    "dataset_test = pd.read_csv('../input/test.csv' )\n",
    "print(dataset_train.head(3))\n",
    "X_test = dataset_test.iloc[:, 1:13]\n",
    "n = X_test.shape[0]\n",
    "# print(X.head(3))\n",
    "# test train split\n",
    "\n",
    "\n",
    "X_test['sin'] = np.sin( X_test['Aspect'])\n",
    "X_test['cos'] = np.cos( X_test['Aspect'])\n",
    "X_test = X_test.drop(columns =['Aspect'])\n",
    "\n",
    "\n",
    "X_test['climate_zone'] = X_test['Soil_Type'].astype(str).str[0] \n",
    "X_test['geologic'] = X_test['Soil_Type'].astype(str).str[1] \n",
    "X_test['soil'] = X_test['Soil_Type'] % 100\n",
    "X_test['soil'] = X_test['soil'].astype('category')\n",
    "X_test['climate_zone'] = X_test['climate_zone'].astype('category')\n",
    "X_test['geologic'] = X_test['geologic'].astype('category')\n",
    "X_test = X_test.drop(columns =['Soil_Type'])\n",
    "\n",
    "\n",
    "X_test['Wilderness_Area1'] = X_test['Wilderness_Area'].astype('category')\n",
    "X_test = X_test.drop(columns =['Wilderness_Area'])\n",
    "\n",
    "X_test.iloc[:,:11] = X_test.iloc[:,:11].astype('float32')\n",
    "\n",
    "\n",
    "X_test_mm = minmax.transform(X_test.iloc[:,:11]) \n",
    "\n",
    "X_test.iloc[:,:11] = X_test_mm\n",
    "X_test_mm_oh = pd.get_dummies(X_test)\n",
    "\n",
    "\n",
    "output = sess.run(predictions,{X1:X_test_mm_oh})\n",
    "output = np.argmax(output,1)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder().fit(['Aspen', 'Cottonwood/Willow', 'Douglas-fir', 'Krummholz', 'Lodgepole Pine', 'Ponderosa Pine', 'Spruce/Fir'])\n",
    "output = le.inverse_transform(output)\n",
    "        \n",
    "        \n",
    "ids = dataset_test['ID']\n",
    "\n",
    "output = pd.DataFrame({ 'ID' : ids, 'Cover_Type': output })\n",
    "output.to_csv('tanh_submission.csv', index = False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
